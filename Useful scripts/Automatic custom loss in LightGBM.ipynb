{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b411a1",
   "metadata": {},
   "source": [
    "One is commonly interested in using different loss functions. Luckily for us, LightGBM allows us to plug in basically any loss we want.\n",
    "\n",
    "To do that, we also need to provide:\n",
    "\n",
    "* The Jacobian with respect to the model parameters\n",
    "* The Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8766e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "\n",
    "from typing import AnyStr, Callable\n",
    "\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import sympy as sym\n",
    "\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c34b31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - y \\log{\\left(p \\right)} - \\left(1 - y\\right) \\log{\\left(1 - p \\right)}$"
      ],
      "text/plain": [
       "-y*log(p) - (1 - y)*log(1 - p)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "y = sym.symbols('y', real=True, constant=True)\n",
    "p = sym.symbols('p', real=True, positive=True)\n",
    "\n",
    "# loss\n",
    "loss = - y * sym.log(p) - (1-y) * sym.log(1-p)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e443e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall derivative of p, which is a logit\n",
    "dp = p*(1-p)\n",
    "\n",
    "# take derivatives (*dp to manually add chain rule)\n",
    "jacob = sym.simplify(sym.diff(loss, p) * dp) \n",
    "hessian = sym.simplify(sym.diff(jacob, p) * dp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac3b811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p - y$"
      ],
      "text/plain": [
       "p - y"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aa293cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p \\left(1 - p\\right)$"
      ],
      "text/plain": [
       "p*(1 - p)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350bcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (y,p)\n",
    "loss_func    = sym.lambdify(params, loss)\n",
    "jacob_func   = sym.lambdify(params, jacob)\n",
    "hessian_func = sym.lambdify(params, hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0e9136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss:\n",
    "\n",
    "    def __init__(self, \n",
    "                 loss_func: Callable,\n",
    "                 jacob_func: Callable,\n",
    "                 hessian_func: Callable,\n",
    "                 loss_name=\"custom_loss\"):\n",
    "        \n",
    "        self.loss = loss_func\n",
    "        self.jacob = jacob_func\n",
    "        self.hessian = hessian_func\n",
    "\n",
    "    def p(self, y_probs):\n",
    "        return np.clip(y_probs, 1e-15, 1)\n",
    "        \n",
    "    def __call__(self, y_true, y_probs):\n",
    "        p = self.p(y_probs)\n",
    "        return self.loss(y=y_true, p=p) #.mean()\n",
    "\n",
    "    def grad(self, y_true, y_probs):\n",
    "        p = self.p(y_probs)\n",
    "        return self.jacob(y=y_true, p=p)\n",
    "\n",
    "    def hess(self, y_true, y_probs):\n",
    "        p = self.p(y_probs)\n",
    "        return self.hessian(y=y_true, p=p)\n",
    "\n",
    "    def init_score(self, y_true):\n",
    "        from scipy import optimize\n",
    "\n",
    "        res = optimize.minimize_scalar(\n",
    "            lambda p: self(y_true, p).sum(),\n",
    "            bounds=(0, 1),\n",
    "            method='bounded'\n",
    "        )\n",
    "        p = res.x\n",
    "        log_odds = np.log(p / (1 - p))\n",
    "        return log_odds\n",
    "\n",
    "    def lgb_obj(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = expit(preds)\n",
    "        return self.grad(y, p), self.hess(y, p)\n",
    "\n",
    "    def lgb_eval(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = expit(preds)\n",
    "        is_higher_better = False\n",
    "        return 'focal_loss', self(y, p).mean(), is_higher_better\n",
    "\n",
    "    \n",
    "class CustomLossLGBM(lgb.LGBMClassifier):\n",
    "\n",
    "    def __init__(self, custom_loss: CustomLoss, **kwargs):\n",
    "        self.params = kwargs\n",
    "        self.fl = custom_loss\n",
    "        self._other_params = []\n",
    "    \n",
    "    def _fit_optimal_rounds(self, fit_data, max_boost_round, early_stopping_rounds):\n",
    "        \"use this with early_stopping to find optimal number of rounds\"\n",
    "\n",
    "        classifier = lgb.Booster(\n",
    "            params=self.params, \n",
    "            train_set=fit_data,\n",
    "        )\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            results = lgb.cv(\n",
    "                init_model=classifier,\n",
    "                params=self.params, \n",
    "                train_set=fit_data,\n",
    "                nfold=2,\n",
    "                num_boost_round=max_boost_round,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                verbose_eval=False,\n",
    "                fobj=self.fl.lgb_obj,\n",
    "                feval=self.fl.lgb_eval\n",
    "            )\n",
    "        \n",
    "        return len(results['focal_loss-mean'])\n",
    "\n",
    "    def fit(self, X_fit, y_fit, max_boost_round=1000, early_stopping_rounds=20):\n",
    "        \n",
    "        self.init_score = self.fl.init_score(y_fit)\n",
    "\n",
    "        fit_data = lgb.Dataset(\n",
    "            X_fit, y_fit,\n",
    "            init_score=np.full_like(y_fit, self.init_score, dtype=float),\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        \n",
    "        self.optimal_boosting_rounds = self._fit_optimal_rounds(fit_data,\n",
    "                                                                max_boost_round, early_stopping_rounds)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            model = lgb.train(\n",
    "                params=self.params,\n",
    "                num_boost_round=self.optimal_boosting_rounds,\n",
    "                train_set=fit_data,\n",
    "                fobj=self.fl.lgb_obj,\n",
    "                feval=self.fl.lgb_eval\n",
    "            )\n",
    "        \n",
    "        self.model = model\n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        prob_1 =  expit(self.init_score + self.model.predict(X))\n",
    "        prob_0 = 1 - prob_1\n",
    "        return np.array([prob_0, prob_1]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4dd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and benchmarking\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "def make_balanced_problem(n_samples=5000):\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=10, n_informative=8, n_redundant=1, n_repeated=1, \n",
    "                               random_state=10) \n",
    "    return X, y\n",
    "\n",
    "X, y = make_balanced_problem(n_samples=15000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae51383",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLossLGBM(custom_loss=CustomLoss(loss_func, jacob_func, hessian_func),\n",
    "                      learning_rate= 0.1,\n",
    "                      n_estimators=500,\n",
    "                      num_leaves=63,\n",
    "                      n_jobs=10,\n",
    "                      verbose=-1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87be81ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC  : 0.986\n",
      "  AP   : 0.984\n",
      "  Brier: 0.037\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_probs = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('  AUC  : {0:.3f}'.format(roc_auc_score(y_test, y_probs)))\n",
    "print('  AP   : {0:.3f}'.format(average_precision_score(y_test, y_probs)))\n",
    "print('  Brier: {0:.3f}'.format(brier_score_loss(y_test, y_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838157c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
