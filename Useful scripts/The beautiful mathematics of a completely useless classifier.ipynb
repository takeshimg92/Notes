{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c194561",
   "metadata": {},
   "source": [
    "# Study of extreme class imbalance in AUC estimation via the random classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c71090",
   "metadata": {},
   "source": [
    "## Recap: the C statistic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc9d85",
   "metadata": {},
   "source": [
    "Recall that the area under the ROC curve, the ROC AUC (denoted simply as AUC below) can be shown to be equivalent to the C-statistic, ie. it has a probabilistic interpration as\n",
    "\n",
    "$$\\boxed{\\mathrm{AUC} := \\mathbb P(Q_1 \\geq Q_0)}$$\n",
    "\n",
    "where $Q_i = f(X)|Y=i$ is the score distribution for class $i \\in \\{0,1\\}$ under a model $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61ac6c",
   "metadata": {},
   "source": [
    "Consider a dataset $\\{x_i, y_i\\}_{i=1}^N$ with $n$ entries of class 1 and $N - n$ entries of class 0. Denote the index sets for these classes as $I \\equiv \\{i: y_i=1\\}$ and $J \\equiv \\{j: y_j = 0\\}$.\n",
    "\n",
    "Let us write $p_i = f(x_i)$ to be the model-forecasted score for entry $x_i$. Then, the quantity\n",
    "\n",
    "$$\\boxed{\\widehat{\\mathrm{AUC}} := \\frac{1}{n(N-n)} \\sum_{i\\in I}\\sum_{j\\in J} 1_{p_i \\geq p_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85aa3b9",
   "metadata": {},
   "source": [
    "is an estimator for the real AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bf3da",
   "metadata": {},
   "source": [
    "### Example (to show that this is true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535eb22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58da31f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(2000, n_features=20, class_sep=0.6, weights=(0.8,), random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dd87aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(X, y)\n",
    "\n",
    "y_probs = model.predict_proba(X)[:,1]\n",
    "auc_area = roc_auc_score(y, y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80310bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC calculated as an area: 0.93089\n"
     ]
    }
   ],
   "source": [
    "print(\"ROC AUC calculated as an area:\", round(auc_area,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "177e2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "n0, n1 = np.bincount(y)\n",
    "total_sum = 0\n",
    "for pi in y_probs[y==1]:\n",
    "    for pj in y_probs[y==0]:\n",
    "        if pi >= pj:\n",
    "            total_sum += 1\n",
    "auc_estimator = total_sum/(n0*n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff8a93ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC calculated as an area: 0.93089\n"
     ]
    }
   ],
   "source": [
    "print(\"ROC AUC calculated as an area:\", round(auc_estimator,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dd46be",
   "metadata": {},
   "source": [
    "Notice how both results are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26067390",
   "metadata": {},
   "source": [
    "## How far away from the actual AUC does the estimator go?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd7d23",
   "metadata": {},
   "source": [
    "When calculating the AUC from a finite sample, one might wonder how precise is the estimator. This problem can be tackled by bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b0bf5",
   "metadata": {},
   "source": [
    "We want to obtain some feeling for an analytical result, so let us get the simplest possible estimator: a random one, where all scores are uniformly sampled from $[0,1]$. In this context, how likely are we to get a spuriously large AUC?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f454797",
   "metadata": {},
   "source": [
    "In particular, we want to consider an **extreme imbalance scenario** where\n",
    "\n",
    "1. $n$ is small (say, less than 100)\n",
    "\n",
    "2. The ratio $n/N$ is small: $n/N \\ll 1$ (say, 1% or less)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a8c2db",
   "metadata": {},
   "source": [
    "Our intuition tells us that this case should be more interesting than the big-data, balanced one: since one has only a few ($n$) points in class 1, if we are \"lucky\" to get high scores for all of them, the AUC will end up being high. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298aa41",
   "metadata": {},
   "source": [
    "We will use the AUC estimator as defined above. First, we have a trivial result: for all $i \\in I$, $j \\in J$, if $p_i, p_j$ iid distributed as $\\mathrm{Uniform}([0,1])$ then\n",
    "\n",
    "$$1_{p_i \\geq p_j} | p_i \\; \\sim \\;\\mathrm{Bernoulli}(p_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4adad",
   "metadata": {},
   "source": [
    "Since a binomial variable is built from a sum of independent Bernoulli ones, we have a corollary: for all $i \\in I$, \n",
    "\n",
    "$$\\sum_{j \\in J}\\left. 1_{p_i \\geq p_j} \\right| p_i \\; \\sim \\; \\mathrm{Binomial}(N-n, p_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9633a4",
   "metadata": {},
   "source": [
    "Now, for large $N-n$, we may use the normal approximation to the binomial, namely a $\\mathrm{Binomial}(n,p)$ variable converges to a $\\mathcal N(\\mu=np, \\sigma^2 = np(1-p))$ variable as $n$ grows. Hence\n",
    "\n",
    "$$\\sum_{j \\in J}\\left. 1_{p_i \\geq p_j} \\right| p_i \\; \\sim \\; \\mathcal N\\left( (N-n)p_i,  (N-n)p_i (1-p_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc9617",
   "metadata": {},
   "source": [
    "It follows that, for all $i\\in I$,\n",
    "\n",
    "$$Z_i |p_i := \\frac{1}{N-n} \\sum_{j \\in J} \\left.1_{p_i \\geq p_j}\\right| \\, p_i \\;\\sim \\; \\mathcal N \\left(p_i, \\frac{p_i (1-p_i)}{N-n}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44772db",
   "metadata": {},
   "source": [
    "This defines a set of $n$ variables $Z_i$. To obtain their marginal distribution, notice that for any $Z_i$ its PDF is given by\n",
    "\n",
    "$$p_{Z_i}(z) = \\int p(z|p_i) p(p_i) dp_i;$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eaf2d6",
   "metadata": {},
   "source": [
    "but $p_i \\; \\sim \\; \\mathrm{Uniform}([0,1])$, and hence its PDF is just the identity on $[0,1]$. Letting\n",
    "\n",
    "$$f(x|\\mu,\\sigma^2) \\equiv \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left\\{ - \\frac{(x-\\mu)^2}{2\\sigma^2} \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25450303",
   "metadata": {},
   "source": [
    "be the Gaussian PDF, we obtain\n",
    "\n",
    "$$p_{Z_i}(z) = \\int_0^1 f \\left(z \\left. \\frac{}{} \\right|\\, p, \\frac{p(1-p)}{N-n} \\right) dp$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18e6d8",
   "metadata": {},
   "source": [
    "For $N-n$ large (as is our hypothesis), the integrand (as a function of $p$) is basically a very sharp peak centered at $p=z$. In fact, we may approximate it as a Dirac delta function\n",
    "\n",
    "$$f\\left(z \\left. \\frac{}{} \\right|\\, p, \\frac{p(1-p)}{N-n} \\right) \\approx \\delta(z-p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97793a",
   "metadata": {},
   "source": [
    "to obtain\n",
    "\n",
    "$$p_{Z_i}(z) = 1_{z \\in [0,1]} \\quad \\Rightarrow \\quad Z_i \\; \\sim \\; \\mathrm{Uniform}([0,1])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15c30d",
   "metadata": {},
   "source": [
    "> We have also tested this numerically - even for $n/N$ not that small this holds surprisingly well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fb61b",
   "metadata": {},
   "source": [
    "**If we assume all $Z_i$'s are independent among themselves**, the it means that $n \\widehat{\\mathrm{AUC}}$ is the sum of $n$ independent uniform variables: it follows the so-called Irwin-Hall distribution, and we have our most important result:\n",
    "\n",
    "> Notice: we have not proven this independence assumption - it is **not obvious**, and we prove it below.\n",
    "\n",
    "**Theorem**. Let $\\widehat{\\mathrm{AUC}}_\\mathrm{random}$ denote the ROC AUC estimator for a *uniformly random scoring function*. Let there be $n$ instances of class 1 and $N-n$ instances of class 0, where $N\\geq n$. Then\n",
    "\n",
    "$$\\boxed{n \\widehat{\\mathrm{AUC}}_\\mathrm{random} \\; \\sim \\; \\mathrm{IrwinHall}(n)};$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2593ce0",
   "metadata": {},
   "source": [
    "notice that this result **does not depend on $N$ explicitly**; we've only used that $N$ is large and also much larger than $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77d938",
   "metadata": {},
   "source": [
    "As Wikipedia can tell us, if $A \\sim \\mathrm{IrwinHall}(n)$ then\n",
    "\n",
    "$$\\mathbb E[A] = \\frac{n}{2};\\qquad \\mathrm{Var}(A) = \\frac{n}{12};$$\n",
    "\n",
    "for the AUC, this gives \n",
    "\n",
    "$$\\boxed{\\mathbb E[\\widehat{\\mathrm{AUC}}_\\mathrm{random}] = \\frac{1}{2};\\qquad \\mathrm{Var}[\\widehat{\\mathrm{AUC}}_\\mathrm{random}] = \\frac{1}{12 n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983eb999",
   "metadata": {},
   "source": [
    "The first result is not surprising: we know that for a random scoring function the AUC should be 1/2. The second one is more surprising, and shows that as we increase $n$, we get an increasingly more precise AUC at 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ea155",
   "metadata": {},
   "source": [
    "We can now use this result to calculate how likely a statistical fluke is to happen: recall Chebyshev's inequality for any (square-integrable) random variable $X$:\n",
    "\n",
    "$$\\mathbb P(|X - \\mu| \\geq t) \\leq \\frac{\\mathrm{Var} \\, X}{t^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b16591",
   "metadata": {},
   "source": [
    "for our random AUC, this gives\n",
    "\n",
    "$$\\boxed{\\mathbb P \\left( \\left|\\widehat{\\mathrm{AUC}}_\\mathrm{random} - \\frac 12 \\right| \\geq t \\right) \\leq \\frac{1}{12 n t^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f9c1c",
   "metadata": {},
   "source": [
    "**Example**: let $n = 20$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55f8da56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of AUC > 0.6 is less than 41.67%\n",
      "Probability of AUC > 0.7 is less than 10.42%\n",
      "Probability of AUC > 0.8 is less than 4.63%\n",
      "Probability of AUC > 0.9 is less than 2.6%\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "for t in [0.1, 0.2, 0.3, 0.4]:\n",
    "    print(f\"Probability of AUC > {0.5+t} is less than {round(100/(12*n*t**2), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841398e",
   "metadata": {},
   "source": [
    "Notice that there is a significant probability of AUC > 0.7: around 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a69dff",
   "metadata": {},
   "source": [
    "### In real-life models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00202628",
   "metadata": {},
   "source": [
    "For any model which is even slightly better than random, the reasoning above might be expanded: we can probably reach higher AUCs more easily than the random case. To be investigated further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54f923",
   "metadata": {},
   "source": [
    "## Proof that the $Z_i$'s are independent as $N$ grows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab648ed0",
   "metadata": {},
   "source": [
    "Recall that, above, we defined\n",
    "\n",
    "$$Z_i \\equiv Z_i^{(N)} = \\frac{1}{N-n} \\sum_{j\\in J} 1_{p_i \\geq p_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c412354",
   "metadata": {},
   "source": [
    "We have shown that as $N$ grows this variable becomes uniform. We can actually prove something stronger: that **$Z_i$ essentially becomes $p_i$ itself!**\n",
    "\n",
    "The intuition here is that, as $N$ grows large, the $p_j$'s basically cover the whole $[0,1]$ interval, and since $Z_i$ cares only about their aggregated values, it essencially becomes independent of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd86ce",
   "metadata": {},
   "source": [
    "More precisely: let $p_J \\equiv \\{p_j: j \\in J\\}$ be a shorthand for all the scores in class 0. We will show three results:\n",
    "\n",
    "* **Proposition 1:** $Z_i|p_J$ converges on the mean to $p_i$, ie. \n",
    "\n",
    "  $$\\lim_{N\\to\\infty} \\mathbb E\\left[\\left.Z_i^{(N)} - p_i \\;\\right| p_J\\right] = 0$$\n",
    "  \n",
    "* **Corollary**: $Z_i|p_J$ converges in probability to $p_i$, ie. \n",
    "\n",
    "  $$\\lim_{N\\to\\infty} \\mathbb P\\left(\\left.|Z_i^{(N)} - p_i| \\geq a \\;\\right| p_J\\right) = 0,\\qquad \\forall a > 0$$\n",
    "  \n",
    "* **Proposition 2:** $Z_i|p_J$ converges in the mean-squared sense to $p_i$, ie. \n",
    "\n",
    "  $$\\boxed{\\lim_{N\\to\\infty} \\mathbb E\\left[\\left. \\left(Z_i^{(N)} - p_i \\right)^2\\;\\right| p_J\\right] = 0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2412f69",
   "metadata": {},
   "source": [
    "Since $p_i$ is independent of $p_J$, this shows that the (conditional on $p_J$) variable $Z_i$ converges to the (unconditional on $p_J$) variable $p_i$. The problem is so unbalanced that the probabilities of the majority class are essentially \"integrated out\", and only the scores of the minority class remain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4df095",
   "metadata": {},
   "source": [
    "> In what follows, we write $N-n \\equiv M$ to unclutter notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8a714",
   "metadata": {},
   "source": [
    "**Proof of Proposition 1**: by direct evaluation, using that the variable $1_{p_i \\geq p_j}\\,|p_j$ is Bernoulli with parameter $1-p_j$, we get\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb E\\left[\\left.Z_i^{(N)} - p_i \\;\\right| p_J\\right] &= \\frac{1}{M} \\sum_j \\mathbb E[1_{p_i \\geq p_j}\\,|p_j]- \\mathbb E[p_i]\\\\\n",
    "&=\\frac 1M \\sum_j (1-p_j) - \\frac 12\\\\\n",
    "&= \\frac 12 - \\frac 1M \\sum_{j\\in J} p_j\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eea2b5",
   "metadata": {},
   "source": [
    "As $M$ (thus $N$) grows larger, intuitively, the $p_j$ cover $[0,1]$ and so the sum above becomes a Riemann sum. We get\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\lim_{N\\to\\infty}\\mathbb E\\left[\\left.Z_i^{(N)} - p_i \\;\\right| p_J\\right] &= \\frac 12 - \\int_0^1 p\\, dp \\\\\n",
    "&=\\frac 12 - \\frac 12\\\\\n",
    "&= 0\n",
    "\\end{align*}$$\n",
    "\n",
    "as we wanted to prove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa8d9d",
   "metadata": {},
   "source": [
    "**Proof of Corollary**. Recall Markov's inequality: if $X$ is an integrable, non-negative random variable, then\n",
    "\n",
    "$$\\mathbb P(X \\geq t) \\leq \\frac{\\mathbb E[X]}{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f4ba7",
   "metadata": {},
   "source": [
    "Setting $X = \\mathbb E\\left[\\left.Z_i^{(N)} - p_i \\;\\right| p_J\\right]$ and taking the limit gives the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b07877",
   "metadata": {},
   "source": [
    "**Proof of Proposition 2**: we want to prove that \n",
    "\n",
    "$$\\lim_{N\\to\\infty} \\mathbb E\\left[\\left. \\left(Z_i^{(N)} - p_i \\right)^2\\;\\right| p_J\\right] = 0.$$\n",
    "\n",
    "To do that, we compute the square\n",
    "\n",
    "$$\\mathbb E\\left[\\left. \\left(Z_i^{(N)} - p_i \\right)^2\\;\\right| p_J\\right] = \\underbrace{\\mathbb E\\left[\\left.(Z_i^{(N)})^2\\,\\right|p_J\\right]}_\\text{(*)} - 2 \\underbrace{\\mathbb E\\left[\\left. p_i Z_i^{(N)} \\,\\right|p_J\\right]}_\\text{(*)} + \\underbrace{\\mathbb E\\left[\\left.p_i^2\\,\\right|p_J\\right]}_\\text{(***)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bff7e4",
   "metadata": {},
   "source": [
    "We compute one term at a time. $(***)$ is the easiest: since $p_i \\,\\sim\\,\\mathrm{Uniform}([0,1])$ is independent of $p_J$, this is just \n",
    "\n",
    "$$\\mathbb E\\left[\\left.p_i^2\\,\\right|p_J\\right] = \\frac 13$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da810ff9",
   "metadata": {},
   "source": [
    "For $(**)$, we need to explicitly compute the expectation; then, taking the limit, we will find a Riemann sum. We have\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb E\\left[\\left. p_i Z_i^{(N)} \\,\\right|p_J\\right] &= \\frac 1M \\sum_j \\mathbb E\\left[\\left. p_i 1_{p_i \\geq p_j}\\,\\right|p_j\\right] \\\\\n",
    "&= \\frac 1M \\sum_j \\int_0^1 p 1_{p \\geq p_j} \\, dp = \\frac 1M \\sum_j \\int_{p_j}^1 p\\, dp \\\\\n",
    "&= \\frac 1M \\sum_j \\left( \\frac{1-p_j^2}{2}\\right)\\\\\n",
    "&\\xrightarrow[N \\to \\infty]{} \\int_0^1 \\frac{1 - p^2}{2} dp\\\\\n",
    "&= \\frac 13\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcabde0",
   "metadata": {},
   "source": [
    "Finally, for $(*)$, the procedure is technically more involved but basically identical:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb E\\left[\\left.(Z_i^{(N)})^2\\,\\right|p_J\\right] &= \\mathbb E \\left[\\left. \\frac{1}{M^2} \\sum_{j \\in J} \\sum_{k\\in J} 1_{p_i \\geq p_j} 1_{p_i \\geq p_k} \\;\\right|p_J\\right]\\\\\n",
    "&=\\frac{1}{M^2}  \\mathbb E \\left[\\left.\\sum_{j \\in J}  1_{p_i \\geq p_j} + 2 \\sum_j \\sum_{k < j} 1_{p_i \\geq p_j} 1_{p_i \\geq p_k} \\;\\right|p_J\\right]\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6f15b",
   "metadata": {},
   "source": [
    "With no loss of generality, order the $p_j$'s in an ascending order, so that $p_j \\geq p_k$ if $j > k$. We can then simplify the second sum by considering that $p_i$ must be larger than $\\max(p_j, p_k) = p_j$. Hence\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb E\\left[\\left.(Z_i^{(N)})^2\\,\\right|p_J\\right] &= \\frac{1}{M^2}\\left\\{\n",
    "    \\sum_j \\mathbb E\\left[\\left.1_{p_i \\geq p_j}\\,\\right|p_j\\right] +\n",
    "    2 \\sum_j \\sum_{k < j} \\mathbb E\\left[\\left.1_{p_i \\geq p_j}\\,\\right|p_j\\right]\n",
    "    \\right\\}\\\\\n",
    "&= \\frac{1}{M^2}\\left\\{\\sum_j (1-p_j) + 2 \\sum_j \\sum_{k < j} (1-p_j)\\right\\}\\\\\n",
    "&= \\frac{1}{M} \\left[\\frac{1}{M}\\sum_j (1-p_j)\\right] + \\frac{2}{M^2} \\sum_j \\sum_{k < j} (1-p_j)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e547c1",
   "metadata": {},
   "source": [
    "Upon taking the limit, the first term will be killed by the excess $1/M$ term in front; the second one becomes the double sum\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb E\\left[\\left.(Z_i^{(N)})^2\\,\\right|p_J\\right] &\\xrightarrow[N \\to \\infty]{} 0 + 2 \\int_0^1 dp \\int_0^p dx \\, (1-p)\\\\\n",
    "&= 2 \\int_0^1 dp \\, p(1-p)\\\\\n",
    "&= \\frac 13.\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3181faf",
   "metadata": {},
   "source": [
    "Hence, putting all results $(*)$ to $(***)$ together, we get\n",
    "\n",
    "$$\\lim_{N\\to\\infty} \\mathbb E\\left[\\left. \\left(Z_i^{(N)} - p_i \\right)^2\\;\\right| p_J\\right] = \\frac 13 - 2 \\cdot \\frac 13 + \\frac 13 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f151d",
   "metadata": {},
   "source": [
    "This proves Proposition 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ea116",
   "metadata": {},
   "source": [
    "## Future outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3c2f2",
   "metadata": {},
   "source": [
    "Read this paper by Mohri:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc85da4",
   "metadata": {},
   "source": [
    "https://cs.nyu.edu/~mohri/pub/area.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f73bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
